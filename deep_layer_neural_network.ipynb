{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 0 次迭代，成本值为： 0.715731513413713\n",
      "第 100 次迭代，成本值为： 0.6747377593469114\n",
      "第 200 次迭代，成本值为： 0.6603365433622127\n",
      "第 300 次迭代，成本值为： 0.6462887802148751\n",
      "第 400 次迭代，成本值为： 0.6298131216927773\n",
      "第 500 次迭代，成本值为： 0.606005622926534\n",
      "第 600 次迭代，成本值为： 0.5690041263975134\n",
      "第 700 次迭代，成本值为： 0.519796535043806\n",
      "第 800 次迭代，成本值为： 0.46415716786282285\n",
      "第 900 次迭代，成本值为： 0.40842030048298916\n",
      "第 1000 次迭代，成本值为： 0.37315499216069026\n",
      "第 1100 次迭代，成本值为： 0.30572374573047106\n",
      "第 1200 次迭代，成本值为： 0.26810152847740837\n",
      "第 1300 次迭代，成本值为： 0.23872474827672654\n",
      "第 1400 次迭代，成本值为： 0.20632263257914718\n",
      "第 1500 次迭代，成本值为： 0.17943886927493605\n",
      "第 1600 次迭代，成本值为： 0.1579873581880163\n",
      "第 1700 次迭代，成本值为： 0.14240413012274492\n",
      "第 1800 次迭代，成本值为： 0.12865165997888675\n",
      "第 1900 次迭代，成本值为： 0.1124431499816437\n",
      "第 2000 次迭代，成本值为： 0.08505631034982422\n",
      "第 2100 次迭代，成本值为： 0.05758391198616691\n",
      "第 2200 次迭代，成本值为： 0.044567534546991264\n",
      "第 2300 次迭代，成本值为： 0.03808275166600256\n",
      "第 2400 次迭代，成本值为： 0.034410749018419895\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确度为: 0.9952153110047847\n",
      "准确度为: 0.78\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import testCases\n",
    "import lr_utils\n",
    "from dnn_utils import sigmoid, sigmoid_backward,relu,relu_backward\n",
    "#np.random.seed(1)\n",
    "\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    此函数是为了初始化多层网络参数而使用的函数。\n",
    "    参数：\n",
    "        layers_dims - 包含我们网络中每个图层的节点数量的列表\n",
    "\n",
    "    返回：\n",
    "        parameters - 包含参数“W1”，“b1”，...，“WL”，“bL”的字典：\n",
    "                     W1 - 权重矩阵，维度为（layers_dims [1]，layers_dims [1-1]）\n",
    "                     bl - 偏向量，维度为（layers_dims [1]，1）\n",
    "    \"\"\"\n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "    \n",
    "    for i in range(1,L):  #range如果缺省第一个参数，那么从0开始，不计尾\n",
    "        #这个地方使用 / np.sqrt(layers_dims[i - 1])，不然会出现梯度消失的问题\n",
    "        parameters[\"W\" + str(i)] = np.random.randn(layer_dims[i], layers_dims[i - 1]) / np.sqrt(layers_dims[i - 1])  \n",
    "        parameters[\"b\" + str(i)] = np.zeros((layer_dims[i], 1))\n",
    "        \n",
    "    return parameters\n",
    "# #测试initialize_parameters_deep\n",
    "# print(\"==============测试initialize_parameters_deep==============\")\n",
    "# layers_dims = [5,4,3]\n",
    "# parameters = initialize_parameters_deep(layers_dims)\n",
    "# print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "# print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "# print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "# print(\"b2 = \" + str(parameters[\"b2\"]))\n",
    "\n",
    "#前向传播线性部分\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    实现前向传播的线性部分。\n",
    "\n",
    "    参数：\n",
    "        A - 来自上一层（或输入数据）的激活，维度为(上一层的节点数量，示例的数量）\n",
    "        W - 权重矩阵，numpy数组，维度为（当前图层的节点数量，前一图层的节点数量）\n",
    "        b - 偏向量，numpy向量，维度为（当前图层节点数量，1）\n",
    "\n",
    "    返回：\n",
    "         Z - 激活功能的输入，也称为预激活参数\n",
    "         cache - 一个包含“A”，“W”和“b”的字典，存储这些变量以有效地计算后向传递\n",
    "    \"\"\"\n",
    "    Z = np.dot(W, A) + b\n",
    "    cache = (A , W, b)\n",
    "    return Z, cache\n",
    "        \n",
    "#前向传播激活函数部分\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    实现LINEAR-> ACTIVATION 这一层的前向传播\n",
    "\n",
    "    参数：\n",
    "        A_prev - 来自上一层（或输入层）的激活，维度为(上一层的节点数量，示例数）\n",
    "        W - 权重矩阵，numpy数组，维度为（当前层的节点数量，前一层的大小）\n",
    "        b - 偏向量，numpy阵列，维度为（当前层的节点数量，1）\n",
    "        activation - 选择在此层中使用的激活函数名，字符串类型，【\"sigmoid\" | \"relu\"】\n",
    "\n",
    "    返回：\n",
    "        A - 激活函数的输出，也称为激活后的值\n",
    "        cache - 一个包含“linear_cache”和“activation_cache”的字典，我们需要存储它以有效地计算后向传递\n",
    "    \"\"\"\n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    return A, cache\n",
    "\n",
    "# #测试linear_activation_forward\n",
    "# print(\"==============测试linear_activation_forward==============\")\n",
    "# A_prev, W,b = testCases.linear_activation_forward_test_case()\n",
    "# A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\")\n",
    "# print(\"sigmoid，A = \" + str(A))\n",
    "# A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n",
    "# print(\"ReLU，A = \" + str(A))\n",
    "\n",
    "#前向传播\n",
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    实现[LINEAR-> RELU] *（L-1） - > LINEAR-> SIGMOID计算前向传播，也就是多层网络的前向传播，为后面每一层都执行LINEAR和ACTIVATION\n",
    "\n",
    "    参数：\n",
    "        X - 数据，numpy数组，维度为（输入节点数量，示例数）\n",
    "        parameters - initialize_parameters_deep（）的输出\n",
    "\n",
    "    返回：\n",
    "        AL - 最后的激活值\n",
    "        caches - 包含以下内容的缓存列表：\n",
    "                 linear_relu_forward（）的每个cache（有L-1个，索引为从0到L-2）\n",
    "                 linear_sigmoid_forward（）的cache（只有一个，索引为L-1）\n",
    "    \"\"\"\n",
    "    caches = []         #建立一个列表，缓存。\n",
    "    A = X\n",
    "    L = len(parameters) // 2  #获得参数的个数\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)],parameters['b' + str(l)],\"relu\")\n",
    "        caches.append(cache)\n",
    "    \n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)],parameters['b' + str(L)], 'sigmoid')\n",
    "    caches.append(cache)\n",
    "    return AL, caches\n",
    "# #测试L_model_forward\n",
    "# print(\"==============测试L_model_forward==============\")\n",
    "# X,parameters = testCases.L_model_forward_test_case()\n",
    "# AL,caches = L_model_forward(X,parameters)\n",
    "# print(\"AL = \" + str(AL))\n",
    "# print(\"caches 的长度为 = \" + str(len(caches)))\n",
    "\n",
    "#计算成本\n",
    "def compute_cost(AL, Y):\n",
    "    m = AL.shape[1]\n",
    "    cost = - np.sum(np.multiply(np.log(AL), Y) + np.multiply(np.log(1 - AL), 1-Y))/m\n",
    "    cost = np.squeeze(cost)\n",
    "    return cost\n",
    "# #测试compute_cost\n",
    "# print(\"==============测试compute_cost==============\")\n",
    "# Y,AL = testCases.compute_cost_test_case()\n",
    "# print(\"cost = \" + str(compute_cost(AL, Y)))\n",
    "\n",
    "def linear_backward(dZ,cache):\n",
    "    \"\"\"\n",
    "    为单层实现反向传播的线性部分（第L层）\n",
    "\n",
    "    参数：\n",
    "         dZ - 相对于（当前第l层的）线性输出的成本梯度\n",
    "         cache - 来自当前层前向传播的值的元组（A_prev，W，b）\n",
    "\n",
    "    返回：\n",
    "         dA_prev - 相对于激活（前一层l-1）的成本梯度，与A_prev维度相同\n",
    "         dW - 相对于W（当前层l）的成本梯度，与W的维度相同\n",
    "         db - 相对于b（当前层l）的成本梯度，与b维度相同\n",
    "    \"\"\" \n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    dW = np.dot(dZ, A_prev.T) / m\n",
    "    db = np.sum(dZ,axis = 1, keepdims = True) / m\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    return dA_prev, dW, db\n",
    "# #测试linear_backward\n",
    "# print(\"==============测试linear_backward==============\")\n",
    "# dZ, linear_cache = testCases.linear_backward_test_case()\n",
    "# dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "# print (\"dA_prev = \"+ str(dA_prev))\n",
    "# print (\"dW = \" + str(dW))\n",
    "# print (\"db = \" + str(db))\n",
    "\n",
    "#后向传播激活\n",
    "def linear_activation_backward(dA,cache,activation=\"relu\"):\n",
    "    \"\"\"\n",
    "    实现LINEAR-> ACTIVATION层的后向传播。\n",
    "\n",
    "    参数：\n",
    "         dA - 当前层l的激活后的梯度值\n",
    "         cache - 我们存储的用于有效计算反向传播的值的元组（值为linear_cache，activation_cache）\n",
    "         activation - 要在此层中使用的激活函数名，字符串类型，【\"sigmoid\" | \"relu\"】\n",
    "    返回：\n",
    "         dA_prev - 相对于激活（前一层l-1）的成本梯度值，与A_prev维度相同\n",
    "         dW - 相对于W（当前层l）的成本梯度值，与W的维度相同\n",
    "         db - 相对于b（当前层l）的成本梯度值，与b的维度相同\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ,linear_cache)\n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ,linear_cache)\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "# #测试linear_activation_backward\n",
    "# print(\"==============测试linear_activation_backward==============\")\n",
    "# AL, linear_activation_cache = testCases.linear_activation_backward_test_case()\n",
    "# dA_prev, dW, db = linear_activation_backward(AL, linear_activation_cache, activation = \"sigmoid\")\n",
    "# print (\"sigmoid:\")\n",
    "# print (\"dA_prev = \"+ str(dA_prev))\n",
    "# print (\"dW = \" + str(dW))\n",
    "# print (\"db = \" + str(db) + \"\\n\")\n",
    "# dA_prev, dW, db = linear_activation_backward(AL, linear_activation_cache, activation = \"relu\")\n",
    "# print (\"relu:\")\n",
    "# print (\"dA_prev = \"+ str(dA_prev))\n",
    "# print (\"dW = \" + str(dW))\n",
    "# print (\"db = \" + str(db))\n",
    "\n",
    "def L_model_backward(AL,Y,caches):\n",
    "    \"\"\"\n",
    "    对[LINEAR-> RELU] *（L-1） - > LINEAR - > SIGMOID组执行反向传播，就是多层网络的向后传播\n",
    "\n",
    "    参数：\n",
    "     AL - 概率向量，正向传播的输出（L_model_forward（））\n",
    "     Y - 标签向量（例如：如果不是猫，则为0，如果是猫则为1），维度为（1，数量）\n",
    "     caches - 包含以下内容的cache列表：\n",
    "                 linear_activation_forward（\"relu\"）的cache，不包含输出层\n",
    "                 linear_activation_forward（\"sigmoid\"）的cache\n",
    "\n",
    "    返回：\n",
    "     grads - 具有梯度值的字典\n",
    "              grads [“dA”+ str（l）] = ...\n",
    "              grads [“dW”+ str（l）] = ...\n",
    "              grads [“db”+ str（l）] = ...\n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    dAL = -(np.divide(Y,AL)-np.divide(1-Y,1-AL))\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, \"sigmoid\")\n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 2)], current_cache, \"relu\")\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads\n",
    "# #测试L_model_backward\n",
    "# print(\"==============测试L_model_backward==============\")\n",
    "# AL, Y_assess, caches = testCases.L_model_backward_test_case()\n",
    "# grads = L_model_backward(AL, Y_assess, caches)\n",
    "# print (\"dW1 = \"+ str(grads[\"dW1\"]))\n",
    "# print (\"db1 = \"+ str(grads[\"db1\"]))\n",
    "# print (\"dA1 = \"+ str(grads[\"dA1\"]))\n",
    "\n",
    "#更新参数\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    使用梯度下降更新参数\n",
    "\n",
    "    参数：\n",
    "     parameters - 包含你的参数的字典\n",
    "     grads - 包含梯度值的字典，是L_model_backward的输出\n",
    "\n",
    "    返回：\n",
    "     parameters - 包含更新参数的字典\n",
    "                   参数[“W”+ str（l）] = ...\n",
    "                   参数[“b”+ str（l）] = ...\n",
    "    \"\"\"\n",
    "    L = len(parameters) // 2 #整除\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * grads[\"db\" + str(l + 1)]\n",
    "    return parameters\n",
    "\n",
    "# #测试update_parameters\n",
    "# print(\"==============测试update_parameters==============\")\n",
    "# parameters, grads = testCases.update_parameters_test_case()\n",
    "# parameters = update_parameters(parameters, grads, 0.1)\n",
    "\n",
    "# print (\"W1 = \"+ str(parameters[\"W1\"]))\n",
    "# print (\"b1 = \"+ str(parameters[\"b1\"]))\n",
    "# print (\"W2 = \"+ str(parameters[\"W2\"]))\n",
    "# print (\"b2 = \"+ str(parameters[\"b2\"]))\n",
    "\n",
    "#测试函数\n",
    "def predict(X, y, parameters):\n",
    "    \"\"\"\n",
    "    该函数用于预测L层神经网络的结果，当然也包含两层\n",
    "\n",
    "    参数：\n",
    "     X - 测试集\n",
    "     y - 标签\n",
    "     parameters - 训练模型的参数\n",
    "\n",
    "    返回：\n",
    "     p - 给定数据集X的预测\n",
    "    \"\"\"\n",
    "\n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 # 神经网络的层数\n",
    "    p = np.zeros((1,m))\n",
    "\n",
    "    #根据参数前向传播\n",
    "    probas, caches = L_model_forward(X, parameters)\n",
    "\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "\n",
    "    print(\"准确度为: \"  + str(float(np.sum((p == y))/m)))\n",
    "\n",
    "    return p\n",
    "\n",
    "#整合、搭建多层神经网络\n",
    "def L_layer_model(X, Y, layers_dims, learning_rate=0.0075, num_iterations=3000, print_cost=False,isPlot=True):\n",
    "    \"\"\"\n",
    "    实现一个L层神经网络：[LINEAR-> RELU] *（L-1） - > LINEAR-> SIGMOID。\n",
    "\n",
    "    参数：\n",
    "        X - 输入的数据，维度为(n_x，例子数)\n",
    "        Y - 标签，向量，0为非猫，1为猫，维度为(1,数量)\n",
    "        layers_dims - 层数的向量，维度为(n_y,n_h,···,n_h,n_y)\n",
    "        learning_rate - 学习率\n",
    "        num_iterations - 迭代的次数\n",
    "        print_cost - 是否打印成本值，每100次打印一次\n",
    "        isPlot - 是否绘制出误差值的图谱\n",
    "\n",
    "    返回：\n",
    "     parameters - 模型学习的参数。 然后他们可以用来预测。\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    costs = []\n",
    "\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "\n",
    "    for i in range(0,num_iterations):\n",
    "        AL , caches = L_model_forward(X,parameters)\n",
    "\n",
    "        cost = compute_cost(AL,Y)\n",
    "\n",
    "        grads = L_model_backward(AL,Y,caches)\n",
    "\n",
    "        parameters = update_parameters(parameters,grads,learning_rate)\n",
    "\n",
    "        #打印成本值，如果print_cost=False则忽略\n",
    "        if i % 100 == 0:\n",
    "            #记录成本\n",
    "            costs.append(cost)\n",
    "            #是否打印成本值\n",
    "            if print_cost:\n",
    "                print(\"第\", i ,\"次迭代，成本值为：\" ,np.squeeze(cost))\n",
    "    #迭代完成，根据条件绘制图\n",
    "    if isPlot:\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "    return parameters\n",
    "train_set_x_orig , train_set_y , test_set_x_orig , test_set_y , classes = lr_utils.load_dataset()\n",
    "\n",
    "train_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T \n",
    "test_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T\n",
    "\n",
    "train_x = train_x_flatten / 255\n",
    "train_y = train_set_y\n",
    "test_x = test_x_flatten / 255\n",
    "test_y = test_set_y\n",
    "\n",
    "layers_dims = [12288, 20, 7, 5, 1] #  5-layer model\n",
    "parameters = L_layer_model(train_x, train_y, layers_dims, num_iterations = 2500, print_cost = True,isPlot=True)\n",
    "\n",
    "predictions_train = predict(train_x, train_y, parameters) #训练集\n",
    "predictions_test = predict(test_x, test_y, parameters) #测试集\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
